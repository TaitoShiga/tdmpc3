"""
Model C: GRUæ¨å®šå™¨çµ±åˆç‰ˆTD-MPC2

ã€æ ¸å¿ƒçš„ç‰¹å¾´ã€‘2ãƒ•ã‚§ãƒ¼ã‚ºåˆ†é›¢ + å‹¾é…åˆ†é›¢

ãƒ•ã‚§ãƒ¼ã‚º1: GRUæ¨å®šå™¨ãŒç‰©ç†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¨å®š
  - æå¤±: L_aux = MSE(c_phys_pred, c_phys_true)
  - æ›´æ–°: GRUã®ã¿

ãƒ•ã‚§ãƒ¼ã‚º2: ãƒ—ãƒ©ãƒ³ãƒŠãƒ¼ãŒæ¨å®šã•ã‚ŒãŸç‰©ç†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
  - æå¤±: L_TD-MPC2 (consistency, reward, value, ...)
  - æ›´æ–°: ãƒ—ãƒ©ãƒ³ãƒŠãƒ¼ï¼ˆdynamics, reward, Q, piï¼‰ã®ã¿
  - é‡è¦: c_physã¯detach()ã•ã‚Œã¦ã„ã‚‹

ä½¿ç”¨æ–¹æ³•:
    python train.py task=pendulum-swingup-randomized use_model_c=true seed=0
"""
import torch
import torch.nn.functional as F

from common import math
from common.scale import RunningScale
from common.world_model_model_c import WorldModelC
from common.layers import api_model_conversion
from tensordict import TensorDict


class TDMPC2ModelC(torch.nn.Module):
	"""
	Model Cç‰ˆTD-MPC2ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€‚
	
	GRUæ¨å®šå™¨ + ç‰©ç†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¡ä»¶ä»˜ããƒ—ãƒ©ãƒ³ãƒŠãƒ¼ã‚’çµ±åˆã€‚
	å‹¾é…åˆ†é›¢ã«ã‚ˆã‚Šã€2ã¤ã®å­¦ç¿’ç›®æ¨™ã‚’å®‰å®šã—ã¦ä¸¡ç«‹ã€‚
	"""
	
	def __init__(self, cfg):
		super().__init__()
		self.cfg = cfg
		self.device = torch.device('cuda:0')
		
		# Model Cç”¨WorldModel
		self.model = WorldModelC(cfg).to(self.device)
		
		# ã€é‡è¦ã€‘2ã¤ã®ç‹¬ç«‹ã—ãŸOptimizer
		# 1. GRUæ¨å®šå™¨ç”¨
		self.gru_optim = torch.optim.Adam(
			self.model._physics_estimator.parameters(),
			lr=getattr(cfg, 'gru_lr', 3e-4),
			weight_decay=getattr(cfg, 'gru_weight_decay', 1e-5),
		)
		
		# 2. ãƒ—ãƒ©ãƒ³ãƒŠãƒ¼ç”¨ï¼ˆdynamics, reward, Q, encoderï¼‰
		self.optim = torch.optim.Adam([
			{'params': self.model._encoder.parameters(), 'lr': self.cfg.lr*self.cfg.enc_lr_scale},
			{'params': self.model._dynamics.parameters()},
			{'params': self.model._reward.parameters()},
			{'params': self.model._termination.parameters() if self.cfg.episodic else []},
			{'params': self.model._Qs.parameters()},
			{'params': self.model._task_emb.parameters() if self.cfg.multitask else []},
		], lr=self.cfg.lr, capturable=True)
		
		# 3. Policyç”¨
		self.pi_optim = torch.optim.Adam(
			self.model._pi.parameters(), 
			lr=self.cfg.lr, 
			eps=1e-5, 
			capturable=True
		)
		
		self.model.eval()
		self.scale = RunningScale(cfg)
		
		# å¤§ããªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç©ºé–“ç”¨ã®ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯
		self.cfg.iterations += 2*int(cfg.action_dim >= 20)
		
		# Discount factor
		self.discount = torch.tensor(
			[self._get_discount(ep_len) for ep_len in cfg.episode_lengths], device='cuda:0'
		) if self.cfg.multitask else self._get_discount(cfg.episode_length)
		
		print('Episode length:', cfg.episode_length)
		print('Discount factor:', self.discount)
		
		# MPPIç”¨ã®å‰å›ã®mean
		self._prev_mean = torch.nn.Buffer(
			torch.zeros(self.cfg.horizon, self.cfg.action_dim, device=self.device)
		)
		
		# å±¥æ­´ãƒãƒƒãƒ•ã‚¡ï¼ˆGRUæ¨å®šç”¨ï¼‰
		self.context_length = getattr(cfg, 'context_length', 50)
		self._obs_history = []
		self._action_history = []
		
		# Compileï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
		if cfg.compile:
			print('Compiling update function with torch.compile...')
			self._update = torch.compile(self._update, mode="reduce-overhead")
	
	@property
	def plan(self):
		_plan_val = getattr(self, "_plan_val", None)
		if _plan_val is not None:
			return _plan_val
		if self.cfg.compile:
			plan = torch.compile(self._plan, mode="reduce-overhead")
		else:
			plan = self._plan
		self._plan_val = plan
		return self._plan_val
	
	def _get_discount(self, episode_length):
		"""ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰é•·ã«å¿œã˜ãŸå‰²å¼•ç‡ã‚’è¿”ã™"""
		frac = episode_length / self.cfg.discount_denom
		return min(max((frac-1)/frac, self.cfg.discount_min), self.cfg.discount_max)
	
	def save(self, fp):
		"""ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®state dictã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
		torch.save({
			"model": self.model.state_dict(),
			"gru_optim": self.gru_optim.state_dict(),
			"optim": self.optim.state_dict(),
			"pi_optim": self.pi_optim.state_dict(),
		}, fp)
	
	def load(self, fp):
		"""ä¿å­˜ã•ã‚ŒãŸstate dictã‚’ãƒ­ãƒ¼ãƒ‰"""
		if isinstance(fp, dict):
			state_dict = fp
		else:
			state_dict = torch.load(fp, map_location=torch.get_default_device(), weights_only=False)
		
		# Modelã®ãƒ­ãƒ¼ãƒ‰
		model_dict = state_dict["model"] if "model" in state_dict else state_dict
		model_dict = api_model_conversion(self.model.state_dict(), model_dict)
		self.model.load_state_dict(model_dict, strict=False)
		
		# Optimizerã®ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
		if "gru_optim" in state_dict:
			self.gru_optim.load_state_dict(state_dict["gru_optim"])
		if "optim" in state_dict:
			self.optim.load_state_dict(state_dict["optim"])
		if "pi_optim" in state_dict:
			self.pi_optim.load_state_dict(state_dict["pi_optim"])
		
		return
	
	def load_pretrained_gru(self, fp):
		"""
		äº‹å‰å­¦ç¿’æ¸ˆã¿ã®GRUæ¨å®šå™¨ã‚’ãƒ­ãƒ¼ãƒ‰ã€‚
		
		Args:
			fp: GRUæ¨å®šå™¨ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ‘ã‚¹
		"""
		checkpoint = torch.load(fp, map_location=self.device)
		self.model._physics_estimator.load_state_dict(
			checkpoint['estimator_state_dict']
		)
		print(f'Loaded pretrained GRU from: {fp}')
		print(f'  Val MAE: {checkpoint["val_mae"]:.4f}')
	
	def reset_history(self):
		"""ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰é–‹å§‹æ™‚ã«å±¥æ­´ã‚’ãƒªã‚»ãƒƒãƒˆ"""
		self._obs_history = []
		self._action_history = []
	
	def update_history(self, obs, action):
		"""å±¥æ­´ã‚’æ›´æ–°"""
		self._obs_history.append(obs.cpu().numpy())
		self._action_history.append(action.cpu().numpy())
		
		# context_lengthã‚’è¶…ãˆãŸã‚‰å¤ã„ã‚‚ã®ã‚’å‰Šé™¤
		if len(self._obs_history) > self.context_length:
			self._obs_history.pop(0)
			self._action_history.pop(0)
	
	def get_history_tensor(self):
		"""
		å±¥æ­´ã‚’Tensorã«å¤‰æ›ã€‚
		
		Returns:
			obs_seq: (1, seq_len, obs_dim)
			action_seq: (1, seq_len, action_dim)
		"""
		import numpy as np
		
		obs_seq = np.array(self._obs_history)
		action_seq = np.array(self._action_history)
		
		obs_seq = torch.from_numpy(obs_seq).float().unsqueeze(0).to(self.device)
		action_seq = torch.from_numpy(action_seq).float().unsqueeze(0).to(self.device)
		
		return obs_seq, action_seq
	
	@torch.no_grad()
	def estimate_physics_online(self):
		"""
		ã€ãƒ•ã‚§ãƒ¼ã‚º1ã€‘ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã§ç‰©ç†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¨å®šã€‚
		
		å±¥æ­´ãŒååˆ†ã«æºœã¾ã£ã¦ã„ãªã„å ´åˆã¯ã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¿”ã™ã€‚
		"""
		if len(self._obs_history) < self.context_length:
			# å±¥æ­´ãŒä¸ååˆ†ãªå ´åˆ
			return torch.zeros(1, self.cfg.c_phys_dim, device=self.device)
		
		obs_seq, action_seq = self.get_history_tensor()
		c_phys_pred = self.model.estimate_physics(obs_seq, action_seq)
		
		return c_phys_pred
	
	@torch.no_grad()
	def act(self, obs, t0=False, eval_mode=False, task=None):
		"""
		ã€ãƒ•ã‚§ãƒ¼ã‚º1+2ã€‘ç‰©ç†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¨å®šã—ã¦ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’é¸æŠã€‚
		
		Args:
			obs: ç’°å¢ƒã‹ã‚‰ã®è¦³æ¸¬
			t0: ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®æœ€åˆã®è¦³æ¸¬ã‹ã©ã†ã‹
			eval_mode: è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰
			task: ã‚¿ã‚¹ã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
		
		Returns:
			action: ç’°å¢ƒã§å®Ÿè¡Œã™ã‚‹ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
		"""
		obs = obs.to(self.device, non_blocking=True).unsqueeze(0)
		
		if task is not None:
			task = torch.tensor([task], device=self.device)
		
		# ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰é–‹å§‹æ™‚ã¯å±¥æ­´ã‚’ãƒªã‚»ãƒƒãƒˆ
		if t0:
			self.reset_history()
		
		# ã€ãƒ•ã‚§ãƒ¼ã‚º1ã€‘ç‰©ç†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¨å®š
		c_phys = self.estimate_physics_online()
		
		if self.cfg.mpc:
			action = self.plan(obs, c_phys, t0=t0, eval_mode=eval_mode, task=task).cpu()
		else:
			# MPCã‚’ä½¿ã‚ãªã„å ´åˆï¼ˆãƒãƒªã‚·ãƒ¼ã®ã¿ï¼‰
			z = self.model.encode(obs, task)
			action, info = self.model.pi(z, task, c_phys)
			if eval_mode:
				action = info["mean"]
			action = action[0].cpu()
		
		return action
	
	@torch.no_grad()
	def _estimate_value(self, z, actions, task, c_phys):
		"""æ½œåœ¨çŠ¶æ…‹zã‹ã‚‰å§‹ã¾ã‚‹è»Œé“ã®ä¾¡å€¤ã‚’æ¨å®š"""
		G, discount = 0, 1
		termination = torch.zeros(self.cfg.num_samples, 1, dtype=torch.float32, device=z.device)
		
		for t in range(self.cfg.horizon):
			reward = math.two_hot_inv(self.model.reward(z, actions[t], task, c_phys), self.cfg)
			z = self.model.next(z, actions[t], task, c_phys)
			G = G + discount * (1-termination) * reward
			discount_update = self.discount[torch.tensor(task)] if self.cfg.multitask else self.discount
			discount = discount * discount_update
			if self.cfg.episodic:
				termination = torch.clip(
					termination + (self.model.termination(z, task, c_phys) > 0.5).float(), 
					max=1.
				)
		
		action, _ = self.model.pi(z, task, c_phys)
		return G + discount * (1-termination) * self.model.Q(z, action, task, c_phys, return_type='avg')
	
	@torch.no_grad()
	def _plan(self, obs, c_phys, t0=False, eval_mode=False, task=None):
		"""
		ã€ãƒ•ã‚§ãƒ¼ã‚º2ã€‘å­¦ç¿’ã—ãŸWorld modelã‚’ä½¿ã£ã¦ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç³»åˆ—ã‚’ãƒ—ãƒ©ãƒ³ã€‚
		"""
		# ãƒãƒªã‚·ãƒ¼è»Œé“ã®ã‚µãƒ³ãƒ—ãƒ«
		z = self.model.encode(obs, task)
		
		if self.cfg.num_pi_trajs > 0:
			pi_actions = torch.empty(
				self.cfg.horizon, self.cfg.num_pi_trajs, self.cfg.action_dim, 
				device=self.device
			)
			_z = z.repeat(self.cfg.num_pi_trajs, 1)
			_c_phys = c_phys.repeat(self.cfg.num_pi_trajs, 1)
			
			for t in range(self.cfg.horizon-1):
				pi_actions[t], _ = self.model.pi(_z, task, _c_phys)
				_z = self.model.next(_z, pi_actions[t], task, _c_phys)
			pi_actions[-1], _ = self.model.pi(_z, task, _c_phys)
		
		# çŠ¶æ…‹ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åˆæœŸåŒ–
		z = z.repeat(self.cfg.num_samples, 1)
		c_phys = c_phys.repeat(self.cfg.num_samples, 1)
		
		mean = torch.zeros(self.cfg.horizon, self.cfg.action_dim, device=self.device)
		std = torch.full(
			(self.cfg.horizon, self.cfg.action_dim), 
			self.cfg.max_std, 
			dtype=torch.float, 
			device=self.device
		)
		
		if not t0:
			mean[:-1] = self._prev_mean[1:]
		
		actions = torch.empty(
			self.cfg.horizon, self.cfg.num_samples, self.cfg.action_dim, 
			device=self.device
		)
		if self.cfg.num_pi_trajs > 0:
			actions[:, :self.cfg.num_pi_trajs] = pi_actions
		
		# MPPIåå¾©
		for _ in range(self.cfg.iterations):
			# ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®ã‚µãƒ³ãƒ—ãƒ«
			r = torch.randn(
				self.cfg.horizon, 
				self.cfg.num_samples - self.cfg.num_pi_trajs, 
				self.cfg.action_dim, 
				device=std.device
			)
			actions_sample = mean.unsqueeze(1) + std.unsqueeze(1) * r
			actions_sample = actions_sample.clamp(-1, 1)
			actions[:, self.cfg.num_pi_trajs:] = actions_sample
			
			if self.cfg.multitask:
				actions = actions * self.model._action_masks[task]
			
			# ã‚¨ãƒªãƒ¼ãƒˆã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®è¨ˆç®—
			value = self._estimate_value(z, actions, task, c_phys).nan_to_num(0)
			elite_idxs = torch.topk(value.squeeze(1), self.cfg.num_elites, dim=0).indices
			elite_value, elite_actions = value[elite_idxs], actions[:, elite_idxs]
			
			# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ›´æ–°
			max_value = elite_value.max(0).values
			score = torch.exp(self.cfg.temperature * (elite_value - max_value))
			score = score / score.sum(0)
			mean = (score.unsqueeze(0) * elite_actions).sum(dim=1) / (score.sum(0) + 1e-9)
			std = ((score.unsqueeze(0) * (elite_actions - mean.unsqueeze(1)) ** 2).sum(dim=1) / (score.sum(0) + 1e-9)).sqrt()
			std = std.clamp(self.cfg.min_std, self.cfg.max_std)
			
			if self.cfg.multitask:
				mean = mean * self.model._action_masks[task]
				std = std * self.model._action_masks[task]
		
		# ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®é¸æŠ
		rand_idx = math.gumbel_softmax_sample(score.squeeze(1))
		actions = torch.index_select(elite_actions, 1, rand_idx).squeeze(1)
		a, std = actions[0], std[0]
		
		if not eval_mode:
			a = a + std * torch.randn(self.cfg.action_dim, device=std.device)
		
		self._prev_mean.copy_(mean)
		return a.clamp(-1, 1)
	
	def update_pi(self, zs, task, c_phys):
		"""æ½œåœ¨çŠ¶æ…‹ã®ç³»åˆ—ã‚’ä½¿ã£ã¦ãƒãƒªã‚·ãƒ¼ã‚’æ›´æ–°"""
		action, info = self.model.pi(zs, task, c_phys)
		qs = self.model.Q(zs, action, task, c_phys, return_type='avg', detach=True)
		self.scale.update(qs[0])
		qs = self.scale(qs)
		
		rho = torch.pow(self.cfg.rho, torch.arange(len(qs), device=self.device))
		pi_loss = (-(self.cfg.entropy_coef * info["scaled_entropy"] + qs).mean(dim=(1,2)) * rho).mean()
		pi_loss.backward()
		pi_grad_norm = torch.nn.utils.clip_grad_norm_(self.model._pi.parameters(), self.cfg.grad_clip_norm)
		self.pi_optim.step()
		self.pi_optim.zero_grad(set_to_none=True)
		
		info = TensorDict({
			"pi_loss": pi_loss,
			"pi_grad_norm": pi_grad_norm,
			"pi_entropy": info["entropy"],
			"pi_scaled_entropy": info["scaled_entropy"],
			"pi_scale": self.scale.value,
		})
		return info
	
	@torch.no_grad()
	def _td_target(self, next_z, reward, terminated, task, c_phys):
		"""TD-targetã‚’è¨ˆç®—"""
		action, _ = self.model.pi(next_z, task, c_phys)
		discount = self.discount[task].unsqueeze(-1) if self.cfg.multitask else self.discount
		return reward + discount * (1-terminated) * self.model.Q(
			next_z, action, task, c_phys, return_type='min', target=True
		)
	
	def _update(self, obs, action, reward, terminated, task=None, c_phys_true=None, obs_seq=None, action_seq=None):
		"""
		ã€æ ¸å¿ƒçš„å®Ÿè£…ã€‘å‹¾é…åˆ†é›¢ã«ã‚ˆã‚‹2ãƒ•ã‚§ãƒ¼ã‚ºå­¦ç¿’ã€‚
		
		Args:
			obs: è¦³æ¸¬ç³»åˆ— (horizon+1, batch, obs_dim)
			action: è¡Œå‹•ç³»åˆ— (horizon, batch, action_dim)
			reward: å ±é…¬ç³»åˆ— (horizon, batch, 1)
			terminated: çµ‚äº†ãƒ•ãƒ©ã‚° (horizon, batch, 1)
			task: ã‚¿ã‚¹ã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
			c_phys_true: çœŸã®ç‰©ç†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ (batch, c_phys_dim)
			obs_seq: GRUç”¨ã®è¦³æ¸¬ç³»åˆ— (batch, context_length, obs_dim)
			action_seq: GRUç”¨ã®è¡Œå‹•ç³»åˆ— (batch, context_length, action_dim)
		
		Returns:
			TensorDict: å­¦ç¿’çµ±è¨ˆæƒ…å ±
		"""
		
		# ========================================
		# ãƒ•ã‚§ãƒ¼ã‚º1: GRUæ¨å®šå™¨ã®æ›´æ–°ï¼ˆL_auxï¼‰
		# ========================================
		if obs_seq is not None and action_seq is not None and c_phys_true is not None:
			# GRUã‚’trainãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š
			self.model._physics_estimator.train()
			
			loss_aux, info_aux = self.model.compute_physics_estimation_loss(
				obs_seq, action_seq, c_phys_true
			)
			
			# GRUæ¨å®šå™¨ã®ã¿æ›´æ–°
			self.gru_optim.zero_grad(set_to_none=True)
			loss_aux.backward()
			gru_grad_norm = torch.nn.utils.clip_grad_norm_(
				self.model._physics_estimator.parameters(), 
				self.cfg.grad_clip_norm
			)
			self.gru_optim.step()
		else:
			loss_aux = torch.tensor(0.0, device=self.device)
			info_aux = {'mae': 0.0, 'max_error': 0.0}
			gru_grad_norm = torch.tensor(0.0, device=self.device)
		
		# ========================================
		# ãƒ•ã‚§ãƒ¼ã‚º2: ãƒ—ãƒ©ãƒ³ãƒŠãƒ¼ã®æ›´æ–°ï¼ˆL_TD-MPC2ï¼‰
		# ========================================
		
		# ğŸ”‘ é‡è¦: GRUã§æ¨å®šã—ãŸc_physã‚’detach()
		with torch.no_grad():
			if obs_seq is not None and action_seq is not None:
				c_phys_pred = self.model.estimate_physics(obs_seq, action_seq)
			else:
				# å±¥æ­´ãŒä¸ååˆ†ãªå ´åˆã¯ã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«
				c_phys_pred = torch.zeros(obs.shape[1], self.cfg.c_phys_dim, device=self.device)
		
		c_phys = c_phys_pred.detach()  # â† å‹¾é…ã‚’åˆ‡ã‚‹ï¼
		
		# Targetã®è¨ˆç®—
		with torch.no_grad():
			next_z = self.model.encode(obs[1:], task)
			td_targets = self._td_target(next_z, reward, terminated, task, c_phys)
		
		# æ›´æ–°ã®æº–å‚™
		self.model.train()
		
		# æ½œåœ¨ç©ºé–“ã§ã®ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆ
		zs = torch.empty(
			self.cfg.horizon+1, self.cfg.batch_size, self.cfg.latent_dim, 
			device=self.device
		)
		z = self.model.encode(obs[0], task)
		zs[0] = z
		consistency_loss = 0
		
		for t, (_action, _next_z) in enumerate(zip(action.unbind(0), next_z.unbind(0))):
			z = self.model.next(z, _action, task, c_phys)
			consistency_loss = consistency_loss + F.mse_loss(z, _next_z) * self.cfg.rho**t
			zs[t+1] = z
		
		# äºˆæ¸¬
		_zs = zs[:-1]
		qs = self.model.Q(_zs, action, task, c_phys, return_type='all')
		reward_preds = self.model.reward(_zs, action, task, c_phys)
		
		if self.cfg.episodic:
			termination_pred = self.model.termination(zs[1:], task, c_phys, unnormalized=True)
		
		# æå¤±ã®è¨ˆç®—
		reward_loss, value_loss = 0, 0
		for t, (rew_pred_unbind, rew_unbind, td_targets_unbind, qs_unbind) in enumerate(
			zip(reward_preds.unbind(0), reward.unbind(0), td_targets.unbind(0), qs.unbind(1))
		):
			reward_loss = reward_loss + math.soft_ce(rew_pred_unbind, rew_unbind, self.cfg).mean() * self.cfg.rho**t
			for _, qs_unbind_unbind in enumerate(qs_unbind.unbind(0)):
				value_loss = value_loss + math.soft_ce(qs_unbind_unbind, td_targets_unbind, self.cfg).mean() * self.cfg.rho**t
		
		consistency_loss = consistency_loss / self.cfg.horizon
		reward_loss = reward_loss / self.cfg.horizon
		
		if self.cfg.episodic:
			termination_loss = F.binary_cross_entropy_with_logits(termination_pred, terminated)
		else:
			termination_loss = 0.
		
		value_loss = value_loss / (self.cfg.horizon * self.cfg.num_q)
		total_loss = (
			self.cfg.consistency_coef * consistency_loss +
			self.cfg.reward_coef * reward_loss +
			self.cfg.termination_coef * termination_loss +
			self.cfg.value_coef * value_loss
		)
		
		# ãƒ—ãƒ©ãƒ³ãƒŠãƒ¼ã®æ›´æ–°
		total_loss.backward()
		grad_norm = torch.nn.utils.clip_grad_norm_(
			list(self.model._encoder.parameters()) +
			list(self.model._dynamics.parameters()) +
			list(self.model._reward.parameters()) +
			list(self.model._Qs.parameters()) +
			(list(self.model._termination.parameters()) if self.cfg.episodic else []),
			self.cfg.grad_clip_norm
		)
		self.optim.step()
		self.optim.zero_grad(set_to_none=True)
		
		# ãƒãƒªã‚·ãƒ¼ã®æ›´æ–°
		pi_info = self.update_pi(zs.detach(), task, c_phys)
		
		# Target Q-functionsã®æ›´æ–°
		self.model.soft_update_target_Q()
		
		# å­¦ç¿’çµ±è¨ˆã‚’è¿”ã™
		self.model.eval()
		info = TensorDict({
			"consistency_loss": consistency_loss,
			"reward_loss": reward_loss,
			"value_loss": value_loss,
			"termination_loss": termination_loss,
			"total_loss": total_loss,
			"grad_norm": grad_norm,
			"gru_loss_aux": loss_aux,
			"gru_mae": info_aux['mae'],
			"gru_grad_norm": gru_grad_norm,
		})
		if self.cfg.episodic:
			info.update(math.termination_statistics(torch.sigmoid(termination_pred[-1]), terminated[-1]))
		info.update(pi_info)
		return info.detach().mean()
	
	def update(self, buffer):
		"""
		ãƒ¡ã‚¤ãƒ³ã®æ›´æ–°é–¢æ•°ã€‚
		
		Args:
			buffer: ãƒªãƒ—ãƒ¬ã‚¤ãƒãƒƒãƒ•ã‚¡ï¼ˆc_phys_trueã¨historyã‚’å«ã‚€ï¼‰
		
		Returns:
			dict: å­¦ç¿’çµ±è¨ˆæƒ…å ±
		"""
		# ãƒãƒƒãƒ•ã‚¡ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ«
		obs, action, reward, terminated, task, c_phys_true, obs_seq, action_seq = buffer.sample()
		
		torch.compiler.cudagraph_mark_step_begin()
		return self._update(
			obs, action, reward, terminated, 
			task=task, 
			c_phys_true=c_phys_true,
			obs_seq=obs_seq,
			action_seq=action_seq
		)

